{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe76263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "import oyaml as yaml\n",
    "\n",
    "from modules import get_backbone, get_criterion, module\n",
    "# from modules.erfnet.erfnet_compile import ERFNet\n",
    "from modules.erfnet.erfnet_modified import ERFNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c1e61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERFNetModel(\n",
       "  (downsampler_block_01): DownsamplerBlock(\n",
       "    (conv): Conv2d(3, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (downsampler_block_02): DownsamplerBlock(\n",
       "    (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_01): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.03, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_02): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.03, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_03): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.03, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_04): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.03, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_05): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.03, inplace=False)\n",
       "  )\n",
       "  (downsampler_block_03): DownsamplerBlock(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_06): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(2, 0), dilation=(2, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2), dilation=(1, 2))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_07): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(4, 0), dilation=(4, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 4), dilation=(1, 4))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_08): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(8, 0), dilation=(8, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 8), dilation=(1, 8))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_09): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(16, 0), dilation=(16, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 16), dilation=(1, 16))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_10): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(2, 0), dilation=(2, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2), dilation=(1, 2))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_11): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(4, 0), dilation=(4, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 4), dilation=(1, 4))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_12): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(8, 0), dilation=(8, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 8), dilation=(1, 8))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_non_bottleneck_1d_13): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(16, 0), dilation=(16, 1))\n",
       "    (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 16), dilation=(1, 16))\n",
       "    (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (upsampler_block_01): UpsamplerBlock(\n",
       "    (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dec_non_bottleneck_1d_01): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (dec_non_bottleneck_1d_02): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (upsampler_block_02): UpsamplerBlock(\n",
       "    (conv): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dec_non_bottleneck_1d_03): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (dec_non_bottleneck_1d_04): non_bottleneck_1d(\n",
       "    (conv3x1_1): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_1): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3x1_2): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (conv1x3_2): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (instance_norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (dropout): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (output_conv): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config/erfnet_predict.yaml\") as istream:\n",
    "    cfg = yaml.safe_load(istream)\n",
    "\n",
    "ckpt = torch.load(\"models/semantic-seg-erfnet.ckpt\", map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "state_dict = {k.replace(\"network.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "num_classes = 3 \n",
    "model = ERFNetModel(num_classes=num_classes)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7055fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8352MB Available: 4817MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 3 due to insufficient memory on requested size of 8352 detected for tactic 0x0000000000000004.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8352MB Available: 4823MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 8 due to insufficient memory on requested size of 8352 detected for tactic 0x000000000000003c.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8336MB Available: 4819MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 3 due to insufficient memory on requested size of 8336 detected for tactic 0x0000000000000004.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8336MB Available: 4823MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 7 due to insufficient memory on requested size of 8336 detected for tactic 0x000000000000003c.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8352MB Available: 4824MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 3 due to insufficient memory on requested size of 8352 detected for tactic 0x0000000000000004.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8352MB Available: 4823MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 8 due to insufficient memory on requested size of 8352 detected for tactic 0x000000000000003c.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8336MB Available: 4819MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 3 due to insufficient memory on requested size of 8336 detected for tactic 0x0000000000000004.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Tactic Device request: 8336MB Available: 4820MB. Device memory is insufficient to use tactic.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Skipping tactic 7 due to insufficient memory on requested size of 8336 detected for tactic 0x000000000000003c.\n",
      "Try decreasing the workspace size with IBuilderConfig::setMemoryPoolLimit().\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Convert model to half precision to match input\n",
    "model = model.half()  # This converts all model parameters to float16\n",
    "\n",
    "example_input = torch.randn(1, 3, 1080, 1920).cuda().half()\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "trt_model = torch_tensorrt.compile(\n",
    "    traced_model,\n",
    "    inputs=[torch_tensorrt.Input(example_input.shape, dtype=torch.half)],\n",
    "    enabled_precisions={torch.half}   # or {torch.float16} if hardware supports\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21881a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 1080, 1920])\n",
      "Output dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = trt_model(example_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output dtype: {output.dtype}\")\n",
    "\n",
    "# Save the compiled model\n",
    "torch.jit.save(trt_model, \"erfnet_tensorrt.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10cad371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905a6f0",
   "metadata": {},
   "source": [
    "## Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94326f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.4434,  2.1426,  1.9756,  ...,  3.2715,  3.1465,  2.9863],\n",
       "          [ 2.3438,  2.0430,  1.7471,  ...,  3.1660,  3.0957,  2.9258],\n",
       "          [ 2.0215,  1.6348,  1.4346,  ...,  3.1504,  3.0254,  2.8711],\n",
       "          ...,\n",
       "          [ 0.4229,  0.1033, -0.0266,  ...,  3.4961,  3.5176,  3.1641],\n",
       "          [ 0.4453,  0.2239,  0.1799,  ...,  3.4551,  3.2520,  3.0410],\n",
       "          [ 0.4500,  0.2322,  0.2227,  ...,  3.2129,  3.1777,  2.9668]],\n",
       "\n",
       "         [[-0.4629, -0.2700,  0.5884,  ..., -1.1572, -1.3184, -1.2139],\n",
       "          [-0.3955, -0.2029,  0.7373,  ..., -1.0781, -1.2773, -1.1650],\n",
       "          [ 0.2915,  0.5425,  1.1748,  ..., -1.1113, -1.1709, -1.0684],\n",
       "          ...,\n",
       "          [ 0.1348,  0.3403,  0.3599,  ..., -2.3047, -1.9434, -1.7100],\n",
       "          [ 0.1986,  0.3403,  0.4429,  ..., -2.1465, -1.7178, -1.5801],\n",
       "          [ 0.1937,  0.3320,  0.4136,  ..., -1.9756, -1.6602, -1.5215]],\n",
       "\n",
       "         [[-2.0039, -1.8945, -2.5840,  ..., -2.1445, -1.8633, -1.8057],\n",
       "          [-1.9736, -1.8643, -2.5020,  ..., -2.1172, -1.8545, -1.7969],\n",
       "          [-2.3359, -2.1953, -2.6309,  ..., -2.0664, -1.8887, -1.8369],\n",
       "          ...,\n",
       "          [-0.5840, -0.4700, -0.3621,  ..., -1.2285, -1.6123, -1.4932],\n",
       "          [-0.6670, -0.5879, -0.6475,  ..., -1.3418, -1.5693, -1.4932],\n",
       "          [-0.6709, -0.5928, -0.6646,  ..., -1.2773, -1.5547, -1.4824]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_ts_module = torch.jit.load(\"models/erfnet_half_tensorrt.ts\")\n",
    "input_data = torch.ones([1, 3, 1080, 1920])\n",
    "input_data = input_data.cuda().half()\n",
    "\n",
    "\n",
    "result = trt_ts_module(input_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc80c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "image = Image.open(\"data/ndsu_2025_08_15/predict/images/frame_021821.jpg\")\n",
    "transform = transforms.ToTensor()\n",
    "image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bc6654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1080, 1920])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7dd0f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for 1 image: 0.0087s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = trt_ts_module(input_data)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Total time for 1 image: {round(end - start, 5)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae54170",
   "metadata": {},
   "source": [
    "## Half precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bc84a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for 1 image: 0.01196s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = trt_ts_module(input_data.half())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Total time for 1 image: {round(end - start, 5)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf91cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/erfnet_predict.yaml\") as istream:\n",
    "    cfg = yaml.safe_load(istream)\n",
    "\n",
    "ckpt = torch.load(\"models/semantic-seg-erfnet.ckpt\", map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "state_dict = {k.replace(\"network.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "num_classes = 3 \n",
    "model = ERFNetModel(num_classes=num_classes)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.eval().cuda()\n",
    "cuda_input = image.to(\"cuda\").unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9d10808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for 1 image: 0.04546s\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    result = model(cuda_input)\n",
    "    end = time.time()\n",
    "print(f\"Total time for 1 image: {round(end - start, 5)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fb7bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image from: frame_021825.jpg\n",
      "Successfully saved darkened image to: darkened_image0.2.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "\n",
    "def darken_image(input_path, output_path, factor=0.7):\n",
    "    \"\"\"\n",
    "    Darkens an image to reduce over-exposure and saves the result.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The file path of the image to be processed.\n",
    "        output_path (str): The file path to save the darkened image.\n",
    "        factor (float): The factor by which to darken the image. \n",
    "                        A value less than 1.0 will darken the image,\n",
    "                        while a value greater than 1.0 will brighten it.\n",
    "                        Default is 0.7.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the input file exists\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"Error: Input file not found at '{input_path}'\")\n",
    "            return\n",
    "\n",
    "        # Open the image file\n",
    "        with Image.open(input_path) as img:\n",
    "            print(f\"Reading image from: {input_path}\")\n",
    "\n",
    "            # Create a brightness enhancer object\n",
    "            enhancer = ImageEnhance.Brightness(img)\n",
    "            \n",
    "            # Apply the darkening factor. A factor of 0.7 makes it 30% darker.\n",
    "            # You can change this value to adjust the effect.\n",
    "            darkened_img = enhancer.enhance(factor)\n",
    "            \n",
    "            # Save the new image to the specified output path\n",
    "            darkened_img.save(output_path)\n",
    "            print(f\"Successfully saved darkened image to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your input and output file paths here.\n",
    "    # Replace 'input_image.jpg' with the path to your image.\n",
    "    # The output path will be where the new image is saved.\n",
    "    factor = 0.2\n",
    "    input_file_path = 'frame_021825.jpg'\n",
    "    output_file_path = f'darkened_image{factor}.jpg'\n",
    "\n",
    "    # You can change the 'factor' to a different value between 0.0 and 1.0\n",
    "    # to control the level of darkness.\n",
    "    darken_image(input_file_path, output_file_path, factor=factor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
